{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "In this question we will:\n",
    "\n",
    "- Compute GLCM and LBP texture descriptors for a training dataset of texture images.\n",
    "- Compute GLCM and LBP texture descriptors for a test dataset of texture images.\n",
    "- Train a K-nearest-neighbours (KNN) classifier using the texture descriptors extracted from training images.\n",
    "- Classify the texture descriptors from the test dataset using the KNN classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Compute the texture descriptions for the training images.\n",
    "\n",
    "For each training image, calculate a vector of GLCM features.  Which GLCM features and the set of displacements you choose to you use are up to you (note that displacements for `skimage.feature.graycomatrix()` need to be specified by distances and angles in **radians**).  Experiment with different combinations of displacements and features to obtain the best possible classification rate.  Use conservative choices to begin with until everything is working, then come back and experiemnt.  As described in the Topic 10 lecture notes, use `skimage.feature.graycomatrix()` and `skimage.feature.graycoprops()` to calculate GLCM features.  You'll probably want to use `normed=True` with `graycomatrix()`.  Your GLCM features should be stored as a 120-row array by m-element array, (m will depend on how many different features and displacements you used and whether or not you combine values for different displacements or not, e.g., by taking their mean).  \n",
    "\n",
    "_Hint: Pay close attention to the format of the return values of  `graycomatrix()` and `graycoprops()`._\n",
    "\n",
    "For each training image, calculate the rotationally invariant LBP features using `skiamge.feature.local_binary_pattern()`.  You can experiment with parameters `P` and `R` to get a good classification rate, but probably `P=8` and `R=1` are good enough.   For the `method` parameter, use `'uniform'` which gives you the rotationally-invariant uniform LBP variant we talked about in class.   Remember that `skiamge.feature.local_binary_pattern()` returns an \"LBP Image\", which is an image in which, when P=8, the pixel value is between 0 and 9, and corresponds to one of the ten possible pattern labels.  It's up to you to turn the \"LBP Image\" into a 10-bin histogram, which serves as the feature vector for that image (you can use `numpy.histogram()` for this but again remember to specify `bins` and `range` parameters, and that it returns two things, and you only need the first one). \n",
    "\n",
    "Addionally, calculate the LBP variance feature again using `skimage.feature.local_binary_pattern()` but use `method='var'` instead.  This is the VAR feature we saw in class.  Use the same P and R as before.  Build a 16-bin histogram of the resulting 'LBP-VAR' image; use `range=(0,7000)` with `numpy.hisotgram()` (this is not quite \"correct\", but it's good enough).  Concatenate these with the rotationally invariant LBP features so that you have a 26-element feature vector for each training image.   These should be stored as a 120-row, 26-column array (26 columns assuming P=8).\n",
    "\n",
    "You can do this all in one loop which builds both feature arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 120 of 120\n",
      "Feature Vector Shape: (120, 16)\n",
      "LBP Uniform Feature Vector Shape: (120, 10)\n",
      "LBP Variance Feature Vector Shape: (120, 16)\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import skimage.feature as feature\n",
    "import os as os\n",
    "import pandas as pd\n",
    "\n",
    "# Read training images\n",
    "brodatztraining_path = '/u1/cmpt487-819/data/asn5/brodatztraining/'\n",
    "brodatztraining_files = pd.read_csv(\"/u1/cmpt487-819/data/asn5/brodatztraining.csv\", header=None)\n",
    "brodatztraining_file_list = brodatztraining_files[0].tolist()\n",
    "\n",
    "glcm_training_feature_vector = []\n",
    "lbp_uniform_training_feature_vector = []\n",
    "lbp_variance_training_feature_vector = []\n",
    "properties = ['contrast', 'homogeneity', 'energy', 'correlation']\n",
    "\n",
    "distances = [1, 2,5,10]  \n",
    "angles = [0,np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "\n",
    "for i in range(len(brodatztraining_file_list)):\n",
    "    image_path = os.path.join(brodatztraining_path, brodatztraining_file_list[i])   \n",
    "    image = io.imread(image_path)\n",
    "    \n",
    "    glcm = feature.graycomatrix(image, distances=distances, levels=256, symmetric=True, angles=angles, normed=True)\n",
    "    glcm_features = []\n",
    "    for prop in properties:\n",
    "        glcm_features.extend(feature.graycoprops(glcm, prop).mean(axis=1)) \n",
    "    glcm_training_feature_vector.append(glcm_features)\n",
    "    \n",
    "    \n",
    "\n",
    "    lbp = feature.local_binary_pattern(image, P=24, R=3, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp, bins=np.arange(11), range=(0, 1))\n",
    "    lbp_hist = lbp_hist / lbp_hist.sum()\n",
    "    lbp_uniform_training_feature_vector.append(lbp_hist)\n",
    "    \n",
    "    \n",
    "    lbp_var = feature.local_binary_pattern(image, P=8, R=1, method='var')\n",
    "    lbp_var_hist, _ = np.histogram(lbp_var, bins=16, range=(0, 7000)) \n",
    "    lbp_var_hist = lbp_var_hist / lbp_var_hist.sum()\n",
    "    lbp_variance_training_feature_vector.append(lbp_var_hist)\n",
    "    \n",
    "    \n",
    "    print(f\"\\rIteration {i + 1} of {len(brodatztraining_file_list)}\", end=\"\")\n",
    "\n",
    "glcm_training_feature_vector = np.array(glcm_training_feature_vector)\n",
    "lbp_uniform_training_feature_vector = np.array(lbp_uniform_training_feature_vector)\n",
    "lbp_variance_training_feature_vector = np.array(lbp_variance_training_feature_vector)\n",
    "\n",
    "print(\"\\nFeature Vector Shape:\", glcm_training_feature_vector.shape)\n",
    "print(\"LBP Uniform Feature Vector Shape:\", lbp_uniform_training_feature_vector.shape)\n",
    "print(\"LBP Variance Feature Vector Shape:\", lbp_variance_training_feature_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Compute Test Image Features\n",
    "\n",
    "Compute the exact same features as you did in step 1 for each of the test images.  Store them in the same way (these arrays will just have more rows, specifically 320 rows, one for each testing sample). For GLCM you'll probably have trouble beating 65% classification rate.  For LBP you should be able to get 95% or better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 320 of 320\n",
      "Feature Vector Shape: (320, 16)\n",
      "LBP Uniform Feature Vector Shape: (320, 10)\n",
      "LBP Variance Feature Vector Shape: (320, 16)\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.  \n",
    "brodatztesting_path = '/u1/cmpt487-819/data/asn5/brodatztesting/'\n",
    "brodatztesting_files = pd.read_csv(\"/u1/cmpt487-819/data/asn5/brodatztesting.csv\", header=None)\n",
    "brodatztesting_file_list = brodatztesting_files[0].tolist()\n",
    "\n",
    "glcm_testing_feature_vector = []\n",
    "lbp_uniform_testing_feature_vector = []\n",
    "lbp_variance_testing_feature_vector = []\n",
    "properties = ['contrast', 'homogeneity', 'energy', 'correlation']\n",
    "\n",
    "distances = [1, 2, 5, 10]\n",
    "angles = [0,np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "\n",
    "for i in range(len(brodatztesting_file_list)):\n",
    "    image_path = os.path.join(brodatztesting_path, brodatztesting_file_list[i])\n",
    "    image = io.imread(image_path)\n",
    "    \n",
    "    glcm = feature.graycomatrix(image, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "\n",
    "    glcm_features = []\n",
    "    for prop in properties:\n",
    "        glcm_features.extend(feature.graycoprops(glcm, prop).mean(axis=1)) \n",
    "    \n",
    "    glcm_testing_feature_vector.append(glcm_features)\n",
    "    \n",
    "    lbp = feature.local_binary_pattern(image, P=24, R=3, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp, bins=np.arange(11), range=(0, 1))\n",
    "    lbp_hist = lbp_hist / lbp_hist.sum()\n",
    "    lbp_uniform_testing_feature_vector.append(lbp_hist)\n",
    "    \n",
    "    \n",
    "    lbp_var = feature.local_binary_pattern(image, P=8, R=1, method='var')\n",
    "    lbp_var_hist, _ = np.histogram(lbp_var, bins=16, range=(0, 7000)) \n",
    "    lbp_var_hist = lbp_var_hist / lbp_var_hist.sum()\n",
    "    lbp_variance_testing_feature_vector.append(lbp_var_hist) \n",
    "\n",
    "    print(f\"\\rIteration {i + 1} of {len(brodatztesting_file_list)}\", end=\"\")\n",
    "\n",
    "glcm_testing_feature_vector = np.array(glcm_testing_feature_vector)\n",
    "lbp_uniform_testing_feature_vector = np.array(lbp_uniform_testing_feature_vector)\n",
    "lbp_variance_testing_feature_vector = np.array(lbp_variance_testing_feature_vector)\n",
    "\n",
    "print(\"\\nFeature Vector Shape:\", glcm_testing_feature_vector.shape)\n",
    "print(\"LBP Uniform Feature Vector Shape:\", lbp_uniform_testing_feature_vector.shape)\n",
    "print(\"LBP Variance Feature Vector Shape:\", lbp_variance_testing_feature_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Generate Label Arrays for the Training and Testing Data\n",
    "\n",
    "Use labels 1 for the first class, label 2 for the second class, etc.   This should be easy to do since the filenames are ordered in blocks of 15 or 40 images of each class for training and testing respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for step 3 here.  \n",
    "training_labels = np.zeros(len(brodatztraining_file_list))\n",
    "testing_labels = np.zeros(len(brodatztesting_file_list))\n",
    "\n",
    "training_files_per_class = 15 \n",
    "testing_files_per_class = 40 \n",
    "num_classes = 8 \n",
    "\n",
    "\n",
    "for class_label in range(1, num_classes + 1):\n",
    "    start_idx = (class_label - 1) * training_files_per_class\n",
    "    end_idx = class_label * training_files_per_class\n",
    "    training_labels[start_idx:end_idx] = class_label\n",
    "\n",
    "for class_label in range(1, num_classes + 1):\n",
    "    start_idx = (class_label - 1) * testing_files_per_class\n",
    "    end_idx = class_label * testing_files_per_class\n",
    "    testing_labels[start_idx:end_idx] = class_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:  Train an KNN classifier.  \n",
    "\n",
    "Train an KNN  classifier using your GLCM features.  Train another one using your LBP features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-66 {color: black;}#sk-container-id-66 pre{padding: 0;}#sk-container-id-66 div.sk-toggleable {background-color: white;}#sk-container-id-66 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-66 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-66 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-66 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-66 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-66 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-66 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-66 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-66 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-66 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-66 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-66 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-66 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-66 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-66 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-66 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-66 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-66 div.sk-item {position: relative;z-index: 1;}#sk-container-id-66 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-66 div.sk-item::before, #sk-container-id-66 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-66 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-66 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-66 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-66 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-66 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-66 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-66 div.sk-label-container {text-align: center;}#sk-container-id-66 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-66 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-66\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-66\" type=\"checkbox\" checked><label for=\"sk-estimator-id-66\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.neighbors as knn\n",
    "\n",
    "# Write your code here. This should be quite short.\n",
    "\n",
    "knn_glcm = knn.KNeighborsClassifier(n_neighbors = 5)\n",
    "knn_glcm.fit(glcm_training_feature_vector,training_labels)\n",
    "\n",
    "\n",
    "combined_lbp_training = np.hstack((lbp_uniform_training_feature_vector, lbp_variance_training_feature_vector))\n",
    "combined_lbp_testing = np.hstack((lbp_uniform_testing_feature_vector, lbp_variance_testing_feature_vector))\n",
    "\n",
    "knn_lbp = knn.KNeighborsClassifier(n_neighbors = 5)\n",
    "knn_lbp.fit(combined_lbp_training,training_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:  Predict the classes of the test images\n",
    "\n",
    "Predict the classes of the test images using both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.  Again this should be quite short.\n",
    "\n",
    "testing_labels = testing_labels.astype(int)\n",
    "\n",
    "\n",
    "predicted_labels_glcm = knn_glcm.predict(glcm_testing_feature_vector).astype(int)\n",
    "\n",
    "predicted_labels_lbp = knn_lbp.predict(combined_lbp_testing).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6:  Display Results\n",
    "\n",
    "Display results as in the final step of Question 1.  For each classifier display the image filenames that were incorrectly classified, the confisuion matrix, and the classification rate.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLCM Results:\n",
      "Confusion Matrix:\n",
      "[[40  0  0  0  0  0  0  0]\n",
      " [33  7  0  0  0  0  0  0]\n",
      " [ 2  0 38  0  0  0  0  0]\n",
      " [ 1  0  0 32  0  0  7  0]\n",
      " [ 0  0  0  0 40  0  0  0]\n",
      " [ 0  0  0  0  0 40  0  0]\n",
      " [ 0  0  0  0  0  0 40  0]\n",
      " [ 0  0  0  0  0  0  0 40]]\n",
      "\n",
      "Classification Rate: 86.56%\n",
      "\n",
      "Misclassified Images:\n",
      "patch-203221.png | True: 2 | Predicted: 1\n",
      "patch-204094.png | True: 2 | Predicted: 1\n",
      "patch-205575.png | True: 2 | Predicted: 1\n",
      "patch-206787.png | True: 2 | Predicted: 1\n",
      "patch-208216.png | True: 2 | Predicted: 1\n",
      "patch-208437.png | True: 2 | Predicted: 1\n",
      "patch-208618.png | True: 2 | Predicted: 1\n",
      "patch-209739.png | True: 2 | Predicted: 1\n",
      "patch-210674.png | True: 2 | Predicted: 1\n",
      "patch-212706.png | True: 2 | Predicted: 1\n",
      "patch-213095.png | True: 2 | Predicted: 1\n",
      "patch-213305.png | True: 2 | Predicted: 1\n",
      "patch-214023.png | True: 2 | Predicted: 1\n",
      "patch-216523.png | True: 2 | Predicted: 1\n",
      "patch-217835.png | True: 2 | Predicted: 1\n",
      "patch-218050.png | True: 2 | Predicted: 1\n",
      "patch-221458.png | True: 2 | Predicted: 1\n",
      "patch-227325.png | True: 2 | Predicted: 1\n",
      "patch-227535.png | True: 2 | Predicted: 1\n",
      "patch-228066.png | True: 2 | Predicted: 1\n",
      "patch-231503.png | True: 2 | Predicted: 1\n",
      "patch-231574.png | True: 2 | Predicted: 1\n",
      "patch-232913.png | True: 2 | Predicted: 1\n",
      "patch-234664.png | True: 2 | Predicted: 1\n",
      "patch-236997.png | True: 2 | Predicted: 1\n",
      "patch-241991.png | True: 2 | Predicted: 1\n",
      "patch-243842.png | True: 2 | Predicted: 1\n",
      "patch-244700.png | True: 2 | Predicted: 1\n",
      "patch-248638.png | True: 2 | Predicted: 1\n",
      "patch-250164.png | True: 2 | Predicted: 1\n",
      "patch-251828.png | True: 2 | Predicted: 1\n",
      "patch-257703.png | True: 2 | Predicted: 1\n",
      "patch-260526.png | True: 2 | Predicted: 1\n",
      "patch-311821.png | True: 3 | Predicted: 1\n",
      "patch-328908.png | True: 3 | Predicted: 1\n",
      "patch-407692.png | True: 4 | Predicted: 7\n",
      "patch-417918.png | True: 4 | Predicted: 7\n",
      "patch-449855.png | True: 4 | Predicted: 7\n",
      "patch-451724.png | True: 4 | Predicted: 7\n",
      "patch-455370.png | True: 4 | Predicted: 7\n",
      "patch-462135.png | True: 4 | Predicted: 7\n",
      "patch-463917.png | True: 4 | Predicted: 7\n",
      "patch-464150.png | True: 4 | Predicted: 1\n",
      "\n",
      "LBP Uniform Results:\n",
      "Confusion Matrix:\n",
      "[[40  0  0  0  0  0  0  0]\n",
      " [ 0 40  0  0  0  0  0  0]\n",
      " [ 0  0 40  0  0  0  0  0]\n",
      " [ 0  0  0 40  0  0  0  0]\n",
      " [ 0  0  0  0 40  0  0  0]\n",
      " [ 0  0  0  0  0 40  0  0]\n",
      " [ 0  0  0  0  0  0 40  0]\n",
      " [ 0  0  0  0  0  0  1 39]]\n",
      "\n",
      "Classification Rate: 99.69%\n",
      "\n",
      "Misclassified Images:\n",
      "patch-850990.png | True: 8 | Predicted: 7\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "# Define a confusion matrix function\n",
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        conf_matrix[true - 1][pred - 1] += 1\n",
    "    return conf_matrix\n",
    "\n",
    "# GLCM results\n",
    "conf_matrix_glcm = confusion_matrix(testing_labels, predicted_labels_glcm, num_classes=8)\n",
    "correct_classifications_glcm = np.trace(conf_matrix_glcm)\n",
    "total_samples_glcm = np.sum(conf_matrix_glcm)\n",
    "classification_rate_glcm = correct_classifications_glcm / total_samples_glcm * 100\n",
    "misclassified_indices_glcm = np.where(testing_labels != predicted_labels_glcm)[0]\n",
    "misclassified_files_glcm = [brodatztesting_file_list[i] for i in misclassified_indices_glcm]\n",
    "\n",
    "# LBP Uniform results\n",
    "conf_matrix_lbp = confusion_matrix(testing_labels, predicted_labels_lbp, num_classes=8)\n",
    "correct_classifications_lbp = np.trace(conf_matrix_lbp)\n",
    "total_samples_lbp = np.sum(conf_matrix_lbp)\n",
    "classification_rate_lbp = correct_classifications_lbp / total_samples_lbp * 100\n",
    "misclassified_indices_lbp = np.where(testing_labels != predicted_labels_lbp)[0]\n",
    "misclassified_files_lbp = [brodatztesting_file_list[i] for i in misclassified_indices_lbp]\n",
    "\n",
    "# Display GLCM results\n",
    "print(\"GLCM Results:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_glcm)\n",
    "print(\"\\nClassification Rate: {:.2f}%\".format(classification_rate_glcm))\n",
    "print(\"\\nMisclassified Images:\")\n",
    "for idx in misclassified_indices_glcm:\n",
    "    print(f\"{brodatztesting_file_list[idx]} | True: {testing_labels[idx]} | Predicted: {predicted_labels_glcm[idx]}\")\n",
    "\n",
    "# Display LBP Uniform results\n",
    "print(\"\\nLBP Uniform Results:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_lbp)\n",
    "print(\"\\nClassification Rate: {:.2f}%\".format(classification_rate_lbp))\n",
    "print(\"\\nMisclassified Images:\")\n",
    "for idx in misclassified_indices_lbp:\n",
    "    print(f\"{brodatztesting_file_list[idx]} | True: {testing_labels[idx]} | Predicted: {predicted_labels_lbp[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Reflections\n",
    "\n",
    "Answer the following questions right here in this block:\n",
    "\n",
    "- Discuss the performance difference of the two different texture features.  Hypothesize reasons for observed differenes.\n",
    "\t\n",
    "\t_Your answer:_\n",
    "    \n",
    "    \n",
    "    \t1.\tGLCM Features:\n",
    "\t•\tThe GLCM features provided a classification rate of 86.56%, which is decent but lower compared to LBP features.\n",
    "\t•\tGLCM primarily relies on second-order statistics, capturing the spatial relationships of pixel intensities. However, this can lead to limitations in robustness when rotation or scaling is present in the texture patterns.\n",
    "\t•\tSince the test dataset contained rotated examples of the textures, GLCM struggled because it lacks inherent rotation invariance. Even with multiple angles considered, the texture’s spatial arrangement changes significantly, which can reduce its ability to generalize well.\n",
    "\t2.\tLBP Features:\n",
    "\t•\tLBP features achieved an impressive classification rate of 99.69%.\n",
    "\t•\tLBP inherently includes rotation invariance and grayscale invariance, making it more robust for datasets with varied orientations and intensities. This is particularly advantageous for datasets like this one, which include rotated textures.\n",
    "\t•\tThe inclusion of LBP variance (VAR) further enhanced the discriminative power by accounting for local texture contrast. This contributed to distinguishing finer details that GLCM could not capture effectively.\n",
    "\n",
    "- For each of your two classifiers, discuss the misclassified images.  Were there any classes that were particularly difficult to distinguish?  Do the misclassified images (over all classes) have anything in common that would cause them to be misclassified?  If so what do they ahve in common, and why do you think it is confusing the classifier?\n",
    "\n",
    "\t_Your answer:_\n",
    "    \n",
    "    \n",
    "    \t1.\tGLCM Misclassified Images:\n",
    "\t•\tThe confusion matrix revealed difficulties in distinguishing classes 2 and 1, as well as some confusion between classes 4 and 7.\n",
    "\t•\tThe misclassified images often had textures with subtle spatial differences that GLCM could not capture effectively. For instance:\n",
    "\t•\tClass 2 textures may exhibit patterns similar to Class 1 but slightly rotated or scaled.\n",
    "\t•\tClass 4’s textures may have co-occurrence properties similar to Class 7’s due to overlapping features like edges or periodicity.\n",
    "\t•\tCommon Causes of Misclassification:\n",
    "\t•\tLack of rotation invariance in GLCM features, as the spatial arrangement shifts under rotation.\n",
    "\t•\tOverlapping intensity patterns in different texture classes that cannot be distinguished by co-occurrence matrices alone.\n",
    "\t2.\tLBP Misclassified Images:\n",
    "\t•\tOnly one image (patch-850990.png) was misclassified, with the true label being Class 8 and predicted as Class 7.\n",
    "\t•\tPossible Reasons:\n",
    "\t•\tThe texture in this misclassified image likely shares similar edge orientation or contrast features with Class 7, leading to confusion in the LBP histogram representation.\n",
    "\t•\tAnother reason could be variations in the local texture density or uneven representation of patterns in the image, which caused the LBP histogram bins to overlap in their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
