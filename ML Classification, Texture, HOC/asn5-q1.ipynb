{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "In this question we will:\n",
    "\n",
    "- Implement and test a Histogram of Curvature Scale (HoCS) descriptor for an image.\n",
    "- Extract the HoCS descriptors for the training images in our leaf dataset.\n",
    "- Prepare a K-nearest-neighbours (KNN) classifier using the descriptors extracted from the training dataset.\n",
    "- Extract the HoCS descriptors for the test images in our leaf dataset.\n",
    "- Classify the descriptors from the test images into one of the three leaf shape classes using the KNN classifier prepared earlier.\n",
    "- Reflect on the performance of the KNN classifier and your choices of parameters for the HoCS descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:  Implement the Histogram of Curvature Scale\n",
    "\n",
    "Write a function called HoCS that returns a histogram of curvature scale feature vector for a given region.  The inputs to your function should be:\n",
    "\n",
    "- `B`: a binary that contains exactly one foreground connected component.\n",
    "- `min_scale`: The samllest scale (circle radius) at which to calcluate curvature (must be a positive integer)\n",
    "- `max_scale`: The largest scale (circle radius) at which to calculate curvature (must be an integer greater than `min_scale`)\n",
    "- `increment`: The increment at which intermediate curvatures should be calculated (must be a positive integer)\n",
    "- `num_bins`: The number of bins in the histogram of curvature for a single scale (must be a positive integer)\n",
    "\n",
    "Your function should compute a histogram of curvature for each scale, starting at `min_scale` ending at (at most) `max_scale`, and for intermediate scales at increments of `increment`.  For example, if `min_scale`=4 and `max_scale`=20, and `increment`=3, then the function should compute a histogram of curvature for scales 4, 7, 10, 13, 16, and 19.  Each histogram at each scale should have `num_bins` bins.  Curvature must be computed using the normalized area integral invariant method described on Slide 39 of the Topic 9 lecture notes.  \n",
    "\n",
    "Normalize each histogram at each scale.\n",
    "\n",
    "To keep things straightforward, your function should only consider the main boundary of the input region and ignore the boundaries of holes in the region.\n",
    "\n",
    "After computing the histogram of curvature at each of the specified scales, all of the histograms should be concatenated into a single one-dimensional array (feature vector) and then returned.\n",
    "\n",
    "_Implementation hint:  You can calculate the normalized area integral invariant of each pixel efficiently using linear filtering.  You will find the function `skimage.morphology.disk()` function useful for designing the appropriate filter masks._\n",
    "\n",
    "_Implementation hint:  Most of the heavy lifting here can be done with module functions from `skimage`, `numpy`, and `scipy`.  Many of the functions mentioned in class and in the notes will be useful.  One that we might not have covered, but will be very handy is `numpy.histogram()`.  When use use it, makes sure you specify both the `bins` and `range` optional arguments.  Also note that `numpy.histogram()` returns TWO things.  You only need the first one, so make sure you write your function call like this:_\n",
    "\n",
    "`the_histogram, stuff_you_dont_need = np.histogram(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code your HoCS function here\n",
    "\n",
    "def HoCS(B, min_scale, max_scale, increment, num_bins):\n",
    "    '''\n",
    "    Computes a histogram of curvature scale for the shape in the binary image B.  \n",
    "    Boundary fragments due to holes are ignored.\n",
    "    :param B: A binary image consisting of a single foreground connected component.\n",
    "    :param min_scale: smallest scale to consider (minimum 1)\n",
    "    :param max_scale: largest scale to consider (max_scale > min_scale)\n",
    "    :param increment:  increment on which to compute scales between min_scale and max_scale\n",
    "    :param num_bins: number of bins for the histogram at each scale\n",
    "    :return: 1D array of histograms concatenated together in order of increasing scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Test your HoCS function.\n",
    "\n",
    "Run your `HoCS()` function on `image_0001.png` from leaftraining directory.  Use `min_scale=5`, `max_scale=25`, `increment=10`, `num_bins=10`.  Plot the resulting feature vector as a bar graph.  Set the y-axis limits to be between 0.0 and 1.0.  You should get a result that matches the sample output in the assignment description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "% matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calculate training features.\n",
    "\n",
    "Use your function from Step 1 to compute the HoCS feature for each of the training images.  It is up to you to determine the parameters for the HoCS feature such as `min_scale`, `max_scale`, etc. to maximize the classification rate.  This will require some experimentation.  Slides 19-12 of Topic 12 lecture notes will be helpful here.  \n",
    "\n",
    "Also generate the training labels here (a column-array of numbers indicating which descriptors belong to each class, e.g. use values 1,2,3 to indicate class 1, 2, and 3.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "import pandas as pd\n",
    "\n",
    "# read in the images listed in leaftraining.csv and compute descriptors for them using your HoCS() function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train the KNN classifier using the feature vectors from the training images.\n",
    "\n",
    "You have another opportunity here to optimize parameters.  You can experiment with the options for the KNN classifier (in partiuclar n_neighbors) to try to obtain better classification rates.  But you won't really be able to do this until after step 6, so just use default parameters to start with. \n",
    "\n",
    "Hint: The steps in this notebook are broken up the way they are so that you can adjust the parameters of training the classifier and then go and perform the classfication without having to re-run the calculation of the features in steps 3 and 5.  You can adjust the parameters here in step 4, and then go and re-run the test set in Step 6 without running step 5 over again -- which is good because step 5 will take a while to run.  Of course you will have to recalculate the features each time you restart PyCharm or the Jupyter Notebook server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors as neigh\n",
    "# Train the KNN classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Calculate the testing features.\n",
    "\n",
    "Compute the HoCS features for all of the testing images using the filenames in `leaftesting.csv`.  Use the same HoCS parameters you did in Step 3.  Also generate class labels for the testing image descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the filenames in leaftesting.csv to load each image and process it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Classfiy the testing features.\n",
    "\n",
    "Classify the descriptors you generated from the test images using the KNN classifier you created in Step 4.\n",
    "\n",
    "Determine the classification rate and the confusion matrix by comparing the results of the classifier to the true class labels for each image.  \n",
    "\n",
    "Print out the filenames of incorrectly classified images.\n",
    "\n",
    "Print the confusion matrix (you don't have to print the row/column indicies as in the example in the assignment description), just the rows and columns of the matrix itself.   Confusion matrix is explained in the background section of the assignment PDF document.\n",
    "\n",
    "Print the correct classification rate.  Classification rate is explained in the Topic 12 notes and in the background section of the assignment PDF document.\n",
    "\n",
    "It should be very easy to get a classficiation rate more than 90%; with well-chosen parameters for your HoCS features and the KNN classifier you should be able to get as much as 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Write your code for Step 6 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Reflections\n",
    "\n",
    "Answer the following questions right here in this block:\n",
    "\n",
    "- Discuss your HoCS parameters and how you arrived at them.  Why did you choose the scales and number of histogram bins that you did?  Are there other values that work just as well?   Likely you tested other HoCS parameters that resulted in worse performance before finding the ones that worked best -- what were some of them and why do you think the performance was worse?\n",
    "\n",
    "\t_Your answer:_\n",
    "\n",
    "- Discuss your choice of KNN classifier parameters and how you arrived at them (think about the same types of questions as in the previous point).\n",
    "\n",
    "\t_Your answer:_\n",
    "\n",
    "- Discuss the misclassified images.  Were there any classes that were particularly difficult to distinguish?  Is there anything unusual about any of the misclassified images that would cuase them to be misclassified?  If so, explain\n",
    "\n",
    "\t_Your answer:_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
