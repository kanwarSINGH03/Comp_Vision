{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "In this question we will:\n",
    "\n",
    "- Compute GLCM and LBP texture descriptors for a training dataset of texture images.\n",
    "- Compute GLCM and LBP texture descriptors for a test dataset of texture images.\n",
    "- Train a K-nearest-neighbours (KNN) classifier using the texture descriptors extracted from training images.\n",
    "- Classify the texture descriptors from the test dataset using the KNN classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Compute the texture descriptions for the training images.\n",
    "\n",
    "For each training image, calculate a vector of GLCM features.  Which GLCM features and the set of displacements you choose to you use are up to you (note that displacements for `skimage.feature.graycomatrix()` need to be specified by distances and angles in **radians**).  Experiment with different combinations of displacements and features to obtain the best possible classification rate.  Use conservative choices to begin with until everything is working, then come back and experiemnt.  As described in the Topic 10 lecture notes, use `skimage.feature.graycomatrix()` and `skimage.feature.graycoprops()` to calculate GLCM features.  You'll probably want to use `normed=True` with `graycomatrix()`.  Your GLCM features should be stored as a 120-row array by m-element array, (m will depend on how many different features and displacements you used and whether or not you combine values for different displacements or not, e.g., by taking their mean).  \n",
    "\n",
    "_Hint: Pay close attention to the format of the return values of  `graycomatrix()` and `graycoprops()`._\n",
    "\n",
    "For each training image, calculate the rotationally invariant LBP features using `skiamge.feature.local_binary_pattern()`.  You can experiment with parameters `P` and `R` to get a good classification rate, but probably `P=8` and `R=1` are good enough.   For the `method` parameter, use `'uniform'` which gives you the rotationally-invariant uniform LBP variant we talked about in class.   Remember that `skiamge.feature.local_binary_pattern()` returns an \"LBP Image\", which is an image in which, when P=8, the pixel value is between 0 and 9, and corresponds to one of the ten possible pattern labels.  It's up to you to turn the \"LBP Image\" into a 10-bin histogram, which serves as the feature vector for that image (you can use `numpy.histogram()` for this but again remember to specify `bins` and `range` parameters, and that it returns two things, and you only need the first one). \n",
    "\n",
    "Addionally, calculate the LBP variance feature again using `skimage.feature.local_binary_pattern()` but use `method='var'` instead.  This is the VAR feature we saw in class.  Use the same P and R as before.  Build a 16-bin histogram of the resulting 'LBP-VAR' image; use `range=(0,7000)` with `numpy.hisotgram()` (this is not quite \"correct\", but it's good enough).  Concatenate these with the rotationally invariant LBP features so that you have a 26-element feature vector for each training image.   These should be stored as a 120-row, 26-column array (26 columns assuming P=8).\n",
    "\n",
    "You can do this all in one loop which builds both feature arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Compute Test Image Features\n",
    "\n",
    "Compute the exact same features as you did in step 1 for each of the test images.  Store them in the same way (these arrays will just have more rows, specifically 320 rows, one for each testing sample). For GLCM you'll probably have trouble beating 65% classification rate.  For LBP you should be able to get 95% or better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Generate Label Arrays for the Training and Testing Data\n",
    "\n",
    "Use labels 1 for the first class, label 2 for the second class, etc.   This should be easy to do since the filenames are ordered in blocks of 15 or 40 images of each class for training and testing respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for step 3 here.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:  Train an KNN classifier.  \n",
    "\n",
    "Train an KNN  classifier using your GLCM features.  Train another one using your LBP features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors as knn\n",
    "\n",
    "# Write your code here. This should be quite short.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:  Predict the classes of the test images\n",
    "\n",
    "Predict the classes of the test images using both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.  Again this should be quite short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6:  Display Results\n",
    "\n",
    "Display results as in the final step of Question 1.  For each classifier display the image filenames that were incorrectly classified, the confisuion matrix, and the classification rate.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Reflections\n",
    "\n",
    "Answer the following questions right here in this block:\n",
    "\n",
    "- Discuss the performance difference of the two different texture features.  Hypothesize reasons for observed differenes.\n",
    "\t\n",
    "\t_Your answer:_\n",
    "\n",
    "- For each of your two classifiers, discuss the misclassified images.  Were there any classes that were particularly difficult to distinguish?  Do the misclassified images (over all classes) have anything in common that would cause them to be misclassified?  If so what do they ahve in common, and why do you think it is confusing the classifier?\n",
    "\n",
    "\t_Your answer:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
